{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Satır Sütun işlemleri**"
      ],
      "metadata": {
        "id": "-Xo1VSa2hD_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#add truth labels as a new column\n",
        "real['label'] = 1\n",
        "fake['label'] = 0"
      ],
      "metadata": {
        "id": "kwGqV2P_fuDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop unnecessary columns \n",
        "df = df.drop(['title', 'subject', 'date'], axis=1) #-- axis 0 yatay, axis 1 dikey\n"
      ],
      "metadata": {
        "id": "dZYswo_ohrPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge fake and real data rows\n",
        "df = pd.concat([real,fake])\n",
        "\n",
        "#alternative\n",
        "data = real.append(fake)"
      ],
      "metadata": {
        "id": "ErLrQSI3h6PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "S44NzpF4qByM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Veri setini karma**"
      ],
      "metadata": {
        "id": "hF6Ux8rDilhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle\n",
        "df1 = df.sample(frac=1, random_state=1) # frac between 0-1, sub-sampling"
      ],
      "metadata": {
        "id": "DHnRZBcWiszL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "mbwuE03Srwjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kök kelime işlemleri**"
      ],
      "metadata": {
        "id": "rU_H93Oerp9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "#spacyde yok\n",
        "#--dumb\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmer.stem('apples')\n",
        "#list tanımlanıp döngü içinde kullanılabilir"
      ],
      "metadata": {
        "id": "8p1CD3NQryjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"ate better good\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token, \"|\", token.lemma_) #token_lemma prints unique hash of word\n",
        "\n",
        "#-- attribute ruler ile custom lemmatizaion kuralı eklenebilir"
      ],
      "metadata": {
        "id": "sSC1Uqk8sApc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ZxSBRU4-qP04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Temizleme ve Standartlaştırma**"
      ],
      "metadata": {
        "id": "axGSjfagiGno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text) #harf haricindekileri temizle\n",
        "    text = text.lower() #küçük harfe çevir\n",
        "    text = text.split()\n",
        "    text = ' '.join(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "7WcKw7TTiOuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regex cleaning and lowercase\n",
        "df[\"text\"] = df[\"text\"].apply(clean)\n",
        "df['text']"
      ],
      "metadata": {
        "id": "rhITIiv0iVay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "uz9FU6HMqTuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopword temizleme**"
      ],
      "metadata": {
        "id": "vhzHAPvAqWug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizasyon**"
      ],
      "metadata": {
        "id": "lhJLn7s1jK6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy tokenizer\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "_VT1MxFUjOcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatizer\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "\n",
        "doc = nlp('constituonal')\n",
        "doc.lemma_\n",
        "\n"
      ],
      "metadata": {
        "id": "Caa_kaAVXzbE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}